# -*- coding: utf-8 -*-
"""LLAMA3_HF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HgcSeVkBgHAyV-kdVviqKQ8f-ONyRuBN

meta-llama/Meta-Llama-3.1-405B
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/LLAMA3_HF
!pip install -r requirements.txt

import json
import torch
from transformers import(AutoTokenizer,
                         AutoModelForCausalLM,
                         BitsAndBytesConfig,
                         pipeline)

config_data = json.load(open('config.json'))
HF_TOKEN = config_data["your_huggingface_token_here"]

model_name = 'meta-llama/Meta-Llama-3.1-405B'

# QUANTIZATION CONFIGURATION
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# LOADING THE TOKENIZER AND LLM
tokenizer = AutoTokenizer.from_pretrained(model_name,
                                          token="your_huggingface_token_here")

tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
    token="your_huggingface_token_here",
    rope_scaling={"type": "dynamic", "factor": 8.0},
    trust_remote_code=True
)

text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=128
)

def get_response(prompt):
  sequences = text_generator(prompt)
  gen_text = sequences[0]["generated_text"]
  return gen_text

prompt = "country names from world map"

llama3_response = get_response(prompt)

llama3_response

