# Naresh_IT_05_01_Ollama

This repository introduces Ollama, a lightweight framework to run large language models (LLMs) locally on your machine. It is part of the GenAI integration track in the Full Stack Data Science curriculum. The focus here is on understanding how to set up Ollama, use it with open models like Mistral or LLaMA, and integrate local LLM inference into practical workflows.

In this module, we:

- Understand what Ollama is and how it enables local LLM inference
- Set up Ollama on a personal machine with GPU or CPU support
- Pull and run open-source models like `mistral`, `llama2`, `codellama`, etc.
- Interact with these models through the Ollama command-line interface or HTTP API
- Explore practical GenAI applications using local models, including text generation, code assistance, and summarization
- Learn how to integrate Ollama outputs into Python or Streamlit-based projects

This repo is ideal for those looking to:

- Experiment with GenAI locally without relying on cloud APIs
- Understand the performance and limitations of open-source LLMs
- Build privacy-focused GenAI tools using local inference
- Extend data science workflows with natural language generation or assistance
- Get hands-on experience with emerging AI infrastructure
