# -*- coding: utf-8 -*-
"""LLM_Chunk_GPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WA0TGUTOWrwYOhOHyUMMQX6R_514VfKK
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

!pip install transformers

"""- The term "transformers" in the context of natural language processing (NLP) and machine learning can refer to various models and architectures built upon the original transformer architecture introduced in the paper "Attention is All You Need" by Vaswani et al. in 2017. Here are some key transformer models and variants that have been developed since then:

1. BERT (Bidirectional Encoder Representations from Transformers)
Focuses on understanding context in both directions (left and right) using masked language modeling.
2. GPT (Generative Pre-trained Transformer)
Developed by OpenAI, GPT models (like GPT-2 and GPT-3) are autoregressive models primarily used for text generation.
3. T5 (Text-to-Text Transfer Transformer)
Treats every NLP task as a text-to-text problem, making it very flexible across various applications.
4. RoBERTa (A Robustly Optimized BERT Pretraining Approach)
An improvement over BERT with more training data and different training strategies.
5. XLNet
Combines the ideas of BERT and autoregressive models, allowing for better capturing of context and dependencies.
6. ALBERT (A Lite BERT)
A smaller and more efficient version of BERT that reduces the number of parameters while maintaining performance.
7. DistilBERT
A distilled version of BERT that is smaller and faster while retaining much of its performance.
8. ERNIE (Enhanced Representation through kNowledge Integration)
Developed by Baidu, it incorporates external knowledge to improve language understanding.
9. ELECTRA
Instead of masking tokens like BERT, ELECTRA predicts replaced tokens, leading to more efficient training.
10. DeBERTa (Decoding-enhanced BERT with Disentangled Attention)
Uses a disentangled attention mechanism to improve performance on various NLP tasks.
11. Vision Transformers (ViT)
Adapts the transformer architecture for image processing tasks, treating images as sequences of patches.
12. BART (Bidirectional and Auto-Regressive Transformers)
Combines BERT's bidirectional encoding and GPT's autoregressive decoding for tasks like summarization and translation.
13. LayoutLM
Designed for document understanding, incorporating layout information from scanned documents.
14. Swin Transformer
A hierarchical vision transformer that can be used for both image classification and detection tasks.
15. Transformer-XL
Introduces recurrence to the transformer architecture, allowing it to handle longer sequences more effectively.
These are just some of the prominent transformer models and architectures. The field is rapidly evolving, with new variations and improvements continually being introduced, so the number and types of transformers are continually growing.
"""

from transformers import AutoTokenizer

# Load the tokenizer for a specific model (e.g., GPT-2)
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Tokenize some input text
text = "Hello, how are you?"
tokens = tokenizer(text, return_tensors='pt')
print(tokens)

from transformers import AutoModelForCausalLM

# Load the pre-trained GPT-2 model
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Generate text
input_ids = tokenizer.encode("indian cricket", return_tensors='pt')
output = model.generate(input_ids, max_length=50)
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(generated_text)







