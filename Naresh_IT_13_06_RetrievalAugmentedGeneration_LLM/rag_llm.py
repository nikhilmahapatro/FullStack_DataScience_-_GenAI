# -*- coding: utf-8 -*-
"""RAG_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11zAwC2rc1RnLbN2p8Ti03QS6voLC-crn
"""

!pip install -U chromadb langchain langchain-groq langchain-community \
    langchain-chroma langchain-text-splitters transformers \
    sentence-transformers unstructured "unstructured[pdf]"

#!apt-get install poppler-utils
#!pip install -U langchain-community
#!pip install chromadb
#!pip install langchain-groq

import os
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq

#!pip install --upgrade unstructured[local-inference]

os.environ['GROQ_API_KEY'] = "your_api_key_here"

import requests
from pathlib import Path

file_path = "/content/drive/MyDrive/RAG_LLM/m3-f7.pdf"

# Read the PDF content directly from the file system
with open(file_path, "rb") as f:
    pdf_content = f.read()

!pip install -U langchain-unstructured

from langchain_unstructured import UnstructuredLoader

loader = UnstructuredFileLoader(file_path)

# !apt-get install tesseract-ocr
# !apt-get install libtesseract-dev

# !apt-get install tesseract-ocr-eng

# !python -m nltk.downloader punkt_tab

import nltk
print(nltk.data.find("."))

import os
os.environ['NLTK_DATA'] = '/root/nltk_data'
import nltk
import unstructured

documents = loader.load()
documents

text_splitter = CharacterTextSplitter(chunk_size=1000,
                                      chunk_overlap=100)
texts = text_splitter.split_documents(documents)

type(texts)

len(texts)

texts[2]

embeddings = HuggingFaceEmbeddings()

persist_dir = "vector_db"

vector_db = Chroma.from_documents(documents=texts,
                                 embedding=embeddings,
                                 persist_directory=persist_dir)

retriever = vector_db.as_retriever()

# LLM from groq
llm = ChatGroq(
    model="llama2-70b-4096",  # <-- Change this to an available model
    temperature=0,
    api_key="your_api_key_here"
)

qa_chain = RetrievalQA.from_chain_type(llm=llm,
                                 chain_type="stuff",
                                 retriever=retriever,
                                 return_source_documents=True)

query = "Which manager got the best out of Cristiano??"
result = qa_chain.invoke({"query": query})
result["result"]

